{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import praw\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gpt4all import GPT4All\n",
    "import csv\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bert.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "features = df[['Likes', 'Num_Comments', 'Content_Sentiment_Score', 'Comment_Sentiment_Score', 'Content_Length', 'Comment_Length']]\n",
    "target = df[['Stock_Price']]  # Ensure DataFrame structure to keep 2D array\n",
    "\n",
    "# Feature normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Target variable normalization\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaled = target_scaler.fit_transform(target)\n",
    "\n",
    "# Create time series samples\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        y = data[i+seq_length, -1]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 7\n",
    "X, y = create_sequences(np.hstack((X_scaled, target_scaled)), seq_length)\n",
    "\n",
    "# Split training and test sets\n",
    "train_size = int(len(X) * 0.7)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build single layer LSTM model\n",
    "def build_lstm_model(units, learning_rate, input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(units, return_sequences=False, input_shape=input_shape),\n",
    "        Dropout(0.3),\n",
    "        Dense(100),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Train LSTM model with best parameters\n",
    "best_params = {'units': 150, 'learning_rate': 0.001}\n",
    "final_model = build_lstm_model(best_params['units'], best_params['learning_rate'], (seq_length, X_train.shape[2]))\n",
    "\n",
    "# Add early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train model with batch size of 8\n",
    "history = final_model.fit(X_train, y_train, batch_size=8, epochs=100, validation_split=0.3, callbacks=[early_stopping])\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = final_model.predict(X_test)\n",
    "predictions = target_scaler.inverse_transform(predictions)\n",
    "y_test_actual = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test_actual, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_actual, predictions)\n",
    "r2 = r2_score(y_test_actual, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "colors = sns.color_palette(\"husl\", 3)\n",
    "\n",
    "# Plot actual vs predicted stock prices\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_actual, label='Actual', color=colors[0])\n",
    "plt.plot(predictions, label='Predicted', color=colors[1])\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Actual vs Predicted Stock Prices')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Residual analysis plot\n",
    "residuals = y_test_actual - predictions\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(residuals, label='Residuals', color=colors[2])\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals (Actual - Predicted)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss function changes\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(history.history['loss'], label='Train Loss', color=colors[0])\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color=colors[1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# RESULT ######################\n",
    "# Epoch 1/100\n",
    "# 472/472 [==============================] - 4s 5ms/step - loss: 0.0123 - val_loss: 0.0033\n",
    "# Epoch 2/100\n",
    "# 472/472 [==============================] - 2s 4ms/step - loss: 0.0064 - val_loss: 0.0047\n",
    "# Epoch 3/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0051 - val_loss: 0.0029\n",
    "# Epoch 4/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0047 - val_loss: 0.0019\n",
    "# Epoch 5/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0045 - val_loss: 0.0019\n",
    "# Epoch 6/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0044 - val_loss: 0.0024\n",
    "# Epoch 7/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0041 - val_loss: 0.0019\n",
    "# Epoch 8/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0042 - val_loss: 0.0046\n",
    "# Epoch 9/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0040 - val_loss: 0.0019\n",
    "# Epoch 10/100\n",
    "# 472/472 [==============================] - 2s 4ms/step - loss: 0.0041 - val_loss: 0.0019\n",
    "# Epoch 11/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0019\n",
    "# Epoch 12/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0040 - val_loss: 0.0019\n",
    "# Epoch 13/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0040 - val_loss: 0.0021\n",
    "# Epoch 14/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0039 - val_loss: 0.0020\n",
    "# Epoch 15/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0039 - val_loss: 0.0019\n",
    "# Epoch 16/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0020\n",
    "# Epoch 17/100\n",
    "# 472/472 [==============================] - 2s 4ms/step - loss: 0.0037 - val_loss: 0.0028\n",
    "# Epoch 18/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0038 - val_loss: 0.0018\n",
    "# Epoch 19/100\n",
    "# 472/472 [==============================] - 2s 4ms/step - loss: 0.0037 - val_loss: 0.0021\n",
    "# Epoch 20/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0021\n",
    "# Epoch 21/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0023\n",
    "# Epoch 22/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0024\n",
    "# Epoch 23/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0019\n",
    "# Epoch 24/100\n",
    "# 472/472 [==============================] - 2s 4ms/step - loss: 0.0037 - val_loss: 0.0019\n",
    "# Epoch 25/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0022\n",
    "# Epoch 26/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0020\n",
    "# Epoch 27/100\n",
    "# 472/472 [==============================] - 2s 4ms/step - loss: 0.0036 - val_loss: 0.0019\n",
    "# Epoch 28/100\n",
    "# 472/472 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0027\n",
    "# 73/73 [==============================] - 0s 2ms/step\n",
    "# Mean Squared Error (MSE): 69.60032426210358\n",
    "# Root Mean Squared Error (RMSE): 8.34268087979539\n",
    "# Mean Absolute Error (MAE): 2.5024898821474317\n",
    "# R² Score: 0.9207340218809266\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Here is use the TLBO ################\n",
    "# Load dataset\n",
    "df = pd.read_csv('bert.csv')\n",
    "\n",
    "# Feature selection\n",
    "features = df[['Likes', 'Num_Comments', 'Content_Sentiment_Score', 'Comment_Sentiment_Score', 'Content_Length', 'Comment_Length']]\n",
    "target = df[['Stock_Price']]  # Ensure DataFrame structure to keep 2D array\n",
    "\n",
    "# Feature normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Target variable normalization\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaled = target_scaler.fit_transform(target)\n",
    "\n",
    "seq_length = 7\n",
    "\n",
    "# Create time series samples\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        y = data[i+seq_length, -1]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X, y = create_sequences(np.hstack((X_scaled, target_scaled)), seq_length)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Split training and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Define LSTM model and evaluation function\n",
    "def build_lstm_model(units, learning_rate, input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(units, return_sequences=False, input_shape=input_shape),\n",
    "        Dropout(0.3),\n",
    "        Dense(100),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def evaluate_model(params):\n",
    "    units = int(params['units'])\n",
    "    learning_rate = params['learning_rate']\n",
    "    model = build_lstm_model(units, learning_rate, (seq_length, X_train.shape[2]))\n",
    "    history = model.fit(X_train, y_train, batch_size=8, epochs=50, validation_split=0.2, verbose=0)\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = target_scaler.inverse_transform(predictions)\n",
    "    y_test_actual = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    mse = mean_squared_error(y_test_actual, predictions)\n",
    "    return mse\n",
    "\n",
    "# Initialize population\n",
    "def initialize_population(nPop, param_bounds):\n",
    "    pop = []\n",
    "    for _ in range(nPop):\n",
    "        individual = {\n",
    "            'units': np.random.randint(param_bounds['units'][0], param_bounds['units'][1]),\n",
    "            'learning_rate': np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1])\n",
    "        }\n",
    "        pop.append(individual)\n",
    "    return pop\n",
    "\n",
    "# Teacher phase and learner phase implementations\n",
    "def teacher_phase(pop, teacher, param_bounds):\n",
    "    new_population = []\n",
    "    for individual in pop:\n",
    "        new_individual = individual.copy()\n",
    "        for param in individual:\n",
    "            difference = np.random.rand() * (teacher[param] - individual[param])\n",
    "            new_individual[param] += difference\n",
    "            new_individual[param] = np.clip(new_individual[param], param_bounds[param][0], param_bounds[param][1])\n",
    "        new_population.append(new_individual)\n",
    "    return new_population\n",
    "\n",
    "def learner_phase(pop, param_bounds):\n",
    "    new_population = pop.copy()\n",
    "    for i, learner1 in enumerate(pop):\n",
    "        learner2 = pop[np.random.choice(len(pop))]\n",
    "        new_individual = learner1.copy()\n",
    "        for param in learner1:\n",
    "            if evaluate_model(learner1) < evaluate_model(learner2):\n",
    "                new_individual[param] += np.random.rand() * (learner1[param] - learner2[param])\n",
    "            else:\n",
    "                new_individual[param] += np.random.rand() * (learner2[param] - learner1[param])\n",
    "            new_individual[param] = np.clip(new_individual[param], param_bounds[param][0], param_bounds[param][1])\n",
    "        new_population[i] = new_individual\n",
    "    return new_population\n",
    "\n",
    "# TLBO optimization process\n",
    "nPop = 10  # Population size\n",
    "MaxIt = 30  # Maximum number of iterations\n",
    "param_bounds = {'units': (100, 150), 'learning_rate': (1e-4, 1e-2)}\n",
    "pop = initialize_population(nPop, param_bounds)\n",
    "\n",
    "# Set early stopping MSE threshold\n",
    "mse_threshold = 1e-5  # Stop iteration if MSE is less than or equal to this value\n",
    "best_mse = np.inf\n",
    "best_params = None\n",
    "\n",
    "for it in range(MaxIt):\n",
    "    fitness = [evaluate_model(individual) for individual in pop]\n",
    "    teacher = pop[np.argmin(fitness)]\n",
    "    pop = teacher_phase(pop, teacher, param_bounds)\n",
    "    pop = learner_phase(pop, param_bounds)\n",
    "    \n",
    "    # Update best parameters\n",
    "    current_best_mse = min(fitness)\n",
    "    if current_best_mse < best_mse:\n",
    "        best_mse = current_best_mse\n",
    "        best_params = pop[np.argmin(fitness)]\n",
    "    \n",
    "    # Output current best parameters for each iteration\n",
    "    print(f'Iteration {it + 1}, Best MSE: {best_mse}, Best Params: {best_params}')\n",
    "    \n",
    "    # Early stopping condition: MSE less than or equal to threshold\n",
    "    if best_mse <= mse_threshold:\n",
    "        print(f\"Stopping early at iteration {it + 1} with MSE: {best_mse}\")\n",
    "        break\n",
    "\n",
    "# Output final best parameters and best MSE\n",
    "print(f\"Best MSE: {best_mse}, Best Params: {best_params}\")\n",
    "\n",
    "# Use best parameters to retrain LSTM model\n",
    "final_model = build_lstm_model(int(best_params['units']), best_params['learning_rate'], (seq_length, X_train.shape[2]))\n",
    "final_model.fit(X_train, y_train, batch_size=8, epochs=100, validation_split=0.2)\n",
    "\n",
    "# Model evaluation and visualization\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = final_model.predict(X_test)\n",
    "predictions = target_scaler.inverse_transform(predictions)\n",
    "y_test_actual = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test_actual, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_actual, predictions)\n",
    "r2 = r2_score(y_test_actual, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "colors = sns.color_palette(\"husl\", 3)\n",
    "\n",
    "# Plot actual vs predicted stock prices\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_actual, label='Actual', color=colors[0])\n",
    "plt.plot(predictions, label='Predicted', color=colors[1])\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Actual vs Predicted Stock Prices')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Residual analysis plot\n",
    "residuals = y_test_actual - predictions\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(residuals, label='Residuals', color=colors[2])\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals (Actual - Predicted)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss function changes\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(history.history['loss'], label='Train Loss', color=colors[0])\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color=colors[1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
